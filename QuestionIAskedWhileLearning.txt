1. why do we have a batch size when training data ?
Answer:
In machine learning, the batch size refers to the number of training samples utilized in one iteration (forward and backward pass) of the learning algorithm. During the training of a neural network or any machine learning model using gradient descent-based optimization algorithms (like stochastic gradient descent, mini-batch gradient descent), the data is divided into batches, and these batches are processed iteratively to update the model's parameters.

There are a few reasons why batch sizes are used in training data:

Computational Efficiency: Utilizing batches allows for more efficient computation, especially when working with large datasets. Processing the entire dataset at once might not fit into memory, so dividing it into smaller batches helps in managing memory consumption.

Gradient Estimation: Batch sizes influence how noisy or accurate the estimation of the gradient is. Larger batch sizes tend to provide a more accurate estimation of the gradient of the loss function because they average gradients over more samples. However, smaller batch sizes introduce more randomness and noise into the gradient estimation, which can sometimes help the model avoid local minima and make the optimization process more stochastic.

Regularization Effects: The choice of batch size can have implicit regularization effects. Smaller batch sizes introduce more noise in the parameter updates, which can act as a form of regularization and help prevent overfitting.

Parallelization: Training deep learning models on hardware like GPUs benefits from parallel processing. Splitting the data into batches allows for parallel computations across different batches, accelerating the training process.

The choice of batch size is a hyperparameter and can significantly impact the training process and the resulting model. Smaller batch sizes might converge faster but can be computationally inefficient, while larger batch sizes can lead to more stable gradients but might require more memory and computational resources. Practitioners often experiment with different batch sizes to find a balance between computational efficiency and model performance for a specific task or dataset.

2. What is an activation function in a Neural Network ?

Answer:

An activation function in a neural network is a mathematical operation applied to the output of each neuron in a neural network layer. It introduces non-linearity into the network, enabling it to learn complex patterns in data. The activation function determines whether a neuron should be activated or not based on the weighted sum of its inputs plus a bias.

Common activation functions include:

Sigmoid: It squashes the input values between 0 and 1. However, it has vanishing gradient problems that can slow down learning, especially in deep networks.

ReLU (Rectified Linear Unit): This function outputs the input directly if it's positive, and zero otherwise. It's simpler and computationally efficient, helping alleviate the vanishing gradient problem.

Tanh: Similar to the sigmoid, but squashes input values between -1 and 1, centered around zero.

Softmax: Usually used in the output layer for multi-class classification problems, as it normalizes the outputs into a probability distribution.

The choice of activation function depends on the nature of the problem being solved, the properties of the data, and the architecture of the neural network. Different activation functions have different characteristics and affect the performance and training of the neural network differently.

3.
